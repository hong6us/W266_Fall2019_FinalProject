{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer questions:\n",
    "\n",
    "1. What are the most frequent words based on weights for each article - we did already\n",
    "2. Words clustering\n",
    "3. Sentiment analysis.  Such as most popular....\n",
    "4. Ranking\n",
    "5. Key word search.  What are the most popular boots for winter 2019 - 2020?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
      "Verbs: ['start', 'work', 'drive', 'take', 'tell', 'shake', 'turn', 'talk', 'say']\n",
      "Sebastian Thrun PERSON\n",
      "Google ORG\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun ORG\n",
      "Recode PRODUCT\n",
      "earlier this week DATE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process whole documents\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://www.elle.com/fashion/shopping/a29657264/winter-bag-trends-2019-2020/',\n",
    "      'https://www.elle.com/fashion/personal-style/a29462082/fresh-outfit-ideas-to-wear-to-work/',\n",
    "      'https://www.elle.com/fashion/shopping/a29711557/winter-shoe-trends-2019-2020/',\n",
    "       'http://www.nfl.com/news/story/0ap3000001074509/article/patrick-mahomes-knee-to-start-sunday-vs-titans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Remove unwanted html element and put words in lower case\n",
    "def urltotext(url):\n",
    "\n",
    "    res = requests.get(url)\n",
    "    html_page = res.content\n",
    "    soup = BeautifulSoup(html_page, 'html.parser')\n",
    "    text = soup.find_all(text=True)\n",
    "\n",
    "\n",
    "    # set([t.parent.name for t in text])\n",
    "\n",
    "    output = ''\n",
    "    blacklist = [\n",
    "        '[document]',\n",
    "        'noscript',\n",
    "        'header',\n",
    "        'html',\n",
    "        'meta',\n",
    "        'head', \n",
    "        'input',\n",
    "        'script',\n",
    "        'style',   \n",
    "        # there may be more elements you don't want.\n",
    "    ]\n",
    "\n",
    "    for t in text:\n",
    "        if t.parent.name not in blacklist:\n",
    "            output += '{} '.format(t)\n",
    "#             print(\"\\t\",t.parent.name)\n",
    "#             print('{} '.format(t))\n",
    "\n",
    "    return output.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize new list\n",
    "\n",
    "def prunePOS(part = None):\n",
    "    \n",
    "    if part is None:\n",
    "        # Create tokenizer\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        text = [tokenizer.tokenize(urltotext(url)) for url in urls]\n",
    "\n",
    "    else:  \n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        text = []\n",
    "        for url in urls:\n",
    "            doc = nlp(urltotext(url))\n",
    "            text.append([token.text for token in doc if token.tag_ == part])\n",
    "        \n",
    "    # Add to words_ns all words that are in words but not in sw\n",
    "    corpus = []\n",
    "    sw = nltk.corpus.stopwords.words('english')\n",
    "    for item in text:\n",
    "        corpus.append(\" \".join([word for word in item if word not in sw]))\n",
    "     \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = prunePOS()\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase = False)\n",
    "wordweights = vectorizer.fit_transform(corpus)\n",
    "\n",
    "corpus = prunePOS(\"JJ\")\n",
    "\n",
    "vectorizer_JJ = TfidfVectorizer(lowercase = False)\n",
    "wordweights_JJ = vectorizer_JJ.fit_transform(corpus)\n",
    "\n",
    "\n",
    "# corpus = prunePOS(\"NP\")\n",
    "\n",
    "# vectorizer_NP = TfidfVectorizer(lowercase = False)\n",
    "# wordweights_NP = vectorizer_NP.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.1715621420947476, 'top'),\n",
       " (0.11913242393272831, 'pouch'),\n",
       " (0.08934931794954624, 'soft'),\n",
       " (0.08934931794954624, 'small'),\n",
       " (0.08934931794954624, 'delicate'),\n",
       " (0.08339096441397625, 'favorite'),\n",
       " (0.07332112540782758, 'new'),\n",
       " (0.07188257697837777, 'structured'),\n",
       " (0.07044402854892796, 'next'),\n",
       " (0.06813726924466286, 'digital'),\n",
       " (0.059566211966364156, 'smooth'),\n",
       " (0.059566211966364156, 'modern'),\n",
       " (0.05009715544147855, 'oversized'),\n",
       " (0.048401234128735114, 'cute'),\n",
       " (0.04696268569928531, 'general'),\n",
       " (0.04542484616310857, 'main'),\n",
       " (0.044260217229725285, 'chic'),\n",
       " (0.029783105983182078, 'worthy'),\n",
       " (0.029783105983182078, 'supple'),\n",
       " (0.029783105983182078, 'spacious'),\n",
       " (0.029783105983182078, 'slouchy'),\n",
       " (0.029783105983182078, 'premillennial'),\n",
       " (0.029783105983182078, 'popular'),\n",
       " (0.029783105983182078, 'photographed'),\n",
       " (0.029783105983182078, 'multiple'),\n",
       " (0.029783105983182078, 'minimalistic'),\n",
       " (0.029783105983182078, 'mikado'),\n",
       " (0.029783105983182078, 'micro'),\n",
       " (0.029783105983182078, 'melissa'),\n",
       " (0.029783105983182078, 'luxurious')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#toDense() might potentailly cause memory issue\n",
    "searchindex = vectorizer.get_feature_names().index('bag')\n",
    "searchweight = wordweights[:,searchindex].todense()\n",
    "\n",
    "reweightedwords = np.asarray(searchweight).T @ np.asarray(wordweights_JJ.todense())\n",
    "\n",
    "\n",
    "list(reversed(sorted((w,word) for w,word in zip(reweightedwords.reshape(-1), vectorizer_JJ.get_feature_names()))))[:30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-d0aff0e56f79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Create tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "# Import RegexpTokenizer from nltk.tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "\n",
    "# Create tokens\n",
    "tokens = tokenizer.tokenize(output)\n",
    "tokens[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-610924f05778>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "Counter(tokens)\n",
    "list(reversed(sorted([(count, word) for word, count in Counter(tokens).items()])))[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-e0b530d0e797>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Loop through list tokens and make lower case\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize new list\n",
    "words = []\n",
    "\n",
    "\n",
    "# Loop through list tokens and make lower case\n",
    "for word in tokens:\n",
    "    words.append(word.lower())\n",
    "\n",
    "\n",
    "# Print several items from list as sanity check\n",
    "words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get English stopwords and print some of them\n",
    "sw = nltk.corpus.stopwords.words('english')\n",
    "sw[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize new list\n",
    "words_ns = []\n",
    "\n",
    "# Add to words_ns all words that are in words but not in sw\n",
    "for word in words:\n",
    "    if word not in sw:\n",
    "        words_ns.append(word)\n",
    "\n",
    "# Print several list items as sanity check\n",
    "words_ns[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
